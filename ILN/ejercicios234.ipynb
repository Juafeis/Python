{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios 2, 3 y 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 2: Analiza el texto con la letra de la canción Heroes de David Bowie de 1977 (BowieHeroes.txt) para obtener una representación gráfica del vocabulario utilizado y la frecuencia de aparición de cada palabra. No se deben tener en cuenta símbolos de puntuación ni stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d4ff5028f77d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m dic = ['a', 'ahead', 'ash', 'c', 'can', 'cannon', 'd', 'distance', 'l', \n\u001b[0;32m     10\u001b[0m        'long', 'o', 'only', 'ort', 's', 'see', 'short', 'w', 'we', 'y']\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BowieHeroes.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m     \"\"\"\n\u001b[1;32m--> 835\u001b[1;33m     \u001b[0mresource_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_resource_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m     \u001b[0mresource_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mnormalize_resource_url\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_resource_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# the resource url has no protocol, use the nltk protocol by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36msplit_resource_url\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'file'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/C:/home/nltk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \"\"\"\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresource_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "dic = ['a', 'ahead', 'ash', 'c', 'can', 'cannon', 'd', 'distance', 'l', \n",
    "       'long', 'o', 'only', 'ort', 's', 'see', 'short', 'w', 'we', 'y']\n",
    "tokenizer = nltk.data.load(dic)\n",
    "\n",
    "f = open(\"BowieHeroes.txt\",'r')\n",
    "res = []\n",
    "for line in f:\n",
    "    res += word_tokenize(line)\n",
    "print(res)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "fdist = FreqDist(word.lower() for word in res if word.isalnum() and word not in stop_words)\n",
    "fdist.plot(len(fdist))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 3: Define una función para detectar palabras raras o poco frecuentes en un texto (en inglés).\n",
    "\t1. Obtener el vocabulario utilizado en el texto: palabras en minúsculas (w.isalpha)\n",
    "\t2. Obtener el conjunto de palabras del corpus nltk.corpus.words también en minúsculas (words())\n",
    "\t3. Quedarse con las palabras del primer conjunto que no estén en el segundo (difference)\n",
    "\t4. Devolver este conjunto ordenado \n",
    "\n",
    "Utiliza esta función para obtener el listado de palabras raras o poco frecuentes de los siguientes textos: BowieLetsDance.txt, nltk.corpus.nps_chat y 'shakespeare-macbeth.txt' de nltk.corpus.gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\jmfel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\jmfel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing BowieLetsDance.txt\n",
      "['While', 'colour', 'lights', 'face', 'Sway', 'crowd', 'empty', 'space', 'run,', \"I'll\", 'run', 'hide,', \"we'll\", 'hide', 'Because', 'love', 'Would', 'break', 'heart', 'two', 'Into', 'arms', 'And', 'tremble', 'like', 'flower', 'grace', 'tonight', 'David', 'Bowie', 'Put', 'red', 'shoes', 'blues', 'To', 'song', \"they're\", \"playin'\", 'radio', 'say']\n",
      "\n",
      "Printing nltk.corpus.nps_chat\n",
      "[':@', 'thunder', 'ladis', 'draw', 'penis', 'docs', 'connected', 'fade', 'touching', 'considerably', 'heeeey', 'sharpie', 'hurry', 'bowl', 'blunt', 'bong', 'nashville', 'ogan', 'cams', 'wonna', 'gooo', 'e-bay', 'yeee', 'haw', 'considering', 'ihavehotnips', 'mirror', 'iamahotnip', 'ahah', 'iamahotniplickme', 'appearently', 'iamahotnipwithpics', 'wedding', 'warning', 'jeep', 'charm', 'necklace', 'falling', 'yayayayayyy', 'oooooo']\n",
      "\n",
      "Printing shakespeare-macbeth.txt\n",
      "['william', 'shakespeare', '1603', 'primus', 'scoena', 'raine', 'hurley', 'burley', 'battaile', 'gray', 'malkin', 'padock', 'houer', 'fogge', 'malcome', 'captaine', 'seemeth', 'plight', 'serieant', 'hardie', 'captiuitie', 'broyle', 'swimmers', 'choake', 'mercilesse', 'macdonwald', 'worthie', 'multiplying', 'villanies', 'swarme', 'westerne', 'isles', 'gallowgrosses', 'supply', 'smiling', 'rebells', 'whore', 'disdayning', 'brandisht', 'smoak']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import nps_chat\n",
    "from collections import Counter\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('nps_chat')\n",
    "\n",
    "def less_common(words, number_of_words):\n",
    "    counter = Counter(words)\n",
    "    return sorted(counter, key = counter.get, reverse=False)[:number_of_words]\n",
    "\n",
    "print(\"Printing BowieLetsDance.txt\")\n",
    "f = open(\"BowieLetsDance.txt\",'r')\n",
    "#Splitting\n",
    "res = []\n",
    "for line in f:\n",
    "    res += line.split()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "res = [word for word in res if word not in stop_words]\n",
    "result = less_common(res, 40)\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "print(\"Printing nltk.corpus.nps_chat\")\n",
    "filter_nps_chat = (word.lower() for word in nps_chat.words())\n",
    "res = less_common(filter_nps_chat, 40)\n",
    "print(res)\n",
    "print()\n",
    "\n",
    "print(\"Printing shakespeare-macbeth.txt\")\n",
    "filter_macbeth = (word.lower() for word in gutenberg.words('shakespeare-macbeth.txt') if word.isalnum())\n",
    "res = less_common(filter_macbeth, 40)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 4: Analiza el texto Moby Dick de Herman Melville 1851, que está en el corpus gutenberg (melville-moby_dyck.txt), utilizando tokenizer, stopwords y stemmer y sin considerar simbolos de puntuación. En concreto:\n",
    "¿Cuántas palabras que no sean signos de puntuación ni stopwords tiene?\n",
    "¿Cuál es el stem mas frecuente?\n",
    "¿Cuántos stems aparecen una sola vez?\n",
    "¿Cuáles son los 50 stems más frecuentes y cuál es su frecuencia? Represéntalas gráficamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "english_stop = stopwords.words('english')\n",
    "##words = gutenberg.words('melville-moby_dick.txt')\n",
    "punt = ['(', ')', ',', '.', ':','[',']','-','--',';','!','?','\\\"','\\'','\\`','\\'\\'', '``']\n",
    "words = []\n",
    "frases = gutenberg.sents('melville-moby_dick.txt')\n",
    "for f in frases:\n",
    "#    print(str(f))\n",
    "    for w in word_tokenize(''.join(f)):\n",
    "        if w not in punt and w.lower() not in english_stop:\n",
    "            words.append(w.lower())\n",
    "vocab = set(words)\n",
    "#print(vocab)\n",
    "stemmer = PorterStemmer()\n",
    "stems = []\n",
    "for word in words:\n",
    "    stems.append(stemmer.stem(word))\n",
    "    \n",
    "vocabStems = set(stems)\n",
    "### ¿Cuántos stems distintos aparecen?\n",
    "print(\"No de stems diferentes = \", len(vocabStems))\n",
    "\n",
    "## ¿Cuántas palabras y frases tiene?\n",
    "print(\"No de frases = \", len(frases))\n",
    "print(\"No de palabras = \", len(words))\n",
    "print(\"No de palabras distintas = \", len(vocab))\n",
    "\n",
    "fdist = FreqDist(stems)\n",
    "## ¿Cuál es el stem mas frecuente?\n",
    "print(\"Stem mas frecuente = \", fdist.max())\n",
    "\n",
    "##¿Cuantos stems aparecen una sola vez?\n",
    "print(\"Numero de stems que aparecen una sola vez = \", len(fdist.hapaxes()))\n",
    "\n",
    "##¿Cuáles son los 50 stems más frecuentes y cuál es su frecuencia?\n",
    "print(\"Listado de los 50 stems más comunes y sus frecuencias\")\n",
    "print(fdist.most_common(50)) # fdist.pprint(50)\n",
    "fdist.plot(50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
